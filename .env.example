# Mindweaver Environment Configuration
# Copy this file to .env.local and customize as needed
# All values shown are defaults - the application works without any .env file

# =============================================================================
# Quick Start: Just run ./mindweaver (no config needed!)
# - Combined mode runs on port 9421
# - Databases created in db/ directory
# - Connect to Ollama at localhost:11434 for AI features
# =============================================================================

# =============================================================================
# Runtime Mode
# =============================================================================
# Default set by --mode flag, override with MODE env var
# MODE=combined              # Mind + Brain in one process (default)
# MODE=mind                  # Mind service only
# MODE=brain                 # Brain service only

# =============================================================================
# Ports (when services are used)
# =============================================================================
# Combined mode:
#   PORT=9421               # Single port for both services (defaults to MIND_PORT)
#
# Standalone mode (--mode=mind or --mode=brain):
#   MIND_PORT=9421          # Mind service port
#   BRAIN_PORT=9422         # Brain service port

# =============================================================================
# Databases
# =============================================================================
# MIND_DB_PATH=db/mind.db    # SQLite database for notes/PKM
# BRAIN_DB_PATH=db/brain.db  # SQLite database for AI/conversations

# =============================================================================
# Standalone Mode: Service URLs
# =============================================================================
# Only needed when running --mode=brain (Brain needs to find Mind)
# MIND_SERVICE_URL=http://localhost:9421

# =============================================================================
# LLM Configuration (for AI features)
# =============================================================================
# Requires Ollama or compatible LLM server
# LLM_ENDPOINT=http://localhost:11434  # Ollama endpoint
# LLM_SMALL_MODEL=phi3-mini            # Fast model for routing
# LLM_BIG_MODEL=phi4                   # Powerful model for reasoning

# =============================================================================
# Logging
# =============================================================================
# LOG_LEVEL=INFO             # DEBUG, INFO, WARN, ERROR
# LOG_FORMAT=text            # text or json

# =============================================================================
# Production Settings
# =============================================================================
# ETAG_SALT=                 # Set for persistent ETags across restarts
#                            # Generate: openssl rand -hex 32
